# .github/workflows/find_xgf_urls.yml
name: Find xgf.nu URLs (High Speed & Verbose)

on:
  workflow_dispatch:
    inputs:
      count:
        description: 'Generate and check this many random URLs'
        required: true
        default: 1000
        type: number
      concurrency:
        description: 'Number of concurrent requests (max connections)'
        required: true
        default: 100
        type: number

permissions:
  contents: write

jobs:
  search:
    runs-on: ubuntu-latest
    env:
      COUNT: ${{ github.event.inputs.count }}
      CONCURRENCY: ${{ github.event.inputs.concurrency }}
      TIMEOUT: 5             # 各リクエストのタイムアウト（秒）
      RETRIES: 2             # タイムアウト/エラー時の再試行回数

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          persist-credentials: true

      - name: Set up Python 3.x
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp asyncio tqdm

      - name: Generate & Check URLs (Async + Verbose)
        run: |
          cat > check_urls_async.py << 'EOF'
          import asyncio
          import aiohttp
          import random
          import string
          import os
          from tqdm import tqdm

          COUNT = int(os.getenv('COUNT', '1000'))
          CONCURRENCY = int(os.getenv('CONCURRENCY', '100'))
          TIMEOUT = float(os.getenv('TIMEOUT', '5'))
          RETRIES = int(os.getenv('RETRIES', '2'))

          sem = asyncio.Semaphore(CONCURRENCY)
          hits = []

          async def fetch(session, url):
              for attempt in range(1, RETRIES + 2):
                  try:
                      async with session.head(url, timeout=TIMEOUT) as resp:
                          status = resp.status
                      print(f"[{url}] Attempt {attempt}: HTTP {status}")
                      return status == 200
                  except Exception as e:
                      print(f"[{url}] Attempt {attempt} error: {e!r}")
                      if attempt <= RETRIES:
                          await asyncio.sleep(0.1)
                          continue
                      return False

          async def worker(i, session):
              slug = ''.join(random.choices(string.ascii_letters + string.digits, k=5))
              url = f"https://xgf.nu/{slug}"
              async with sem:
                  if await fetch(session, url):
                      hits.append(url)

          async def main():
              conn = aiohttp.TCPConnector(limit=CONCURRENCY)
              timeout = aiohttp.ClientTimeout(total=None)
              async with aiohttp.ClientSession(connector=conn, timeout=timeout) as session:
                  tasks = [worker(i, session) for i in range(COUNT)]
                  for f in tqdm(asyncio.as_completed(tasks), total=COUNT):
                      await f
              # ファイルに書き出し
              with open('hits.txt', 'w') as f:
                  for u in hits:
                      f.write(f"{u}\n")
              print(f"Total hits: {len(hits)}")

          if __name__ == "__main__":
              asyncio.run(main())
          EOF

          # 古いファイルをクリア
          rm -f hits.txt

          # 実行
          python check_urls_async.py

      - name: Dump logs
        run: |
          echo "===== hits.txt ====="
          cat hits.txt || echo "(no hits)"

      - name: Commit & Push if any hits
        run: |
          if [ -s hits.txt ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add hits.txt
            git commit -m "Update hits.txt (run #${GITHUB_RUN_NUMBER})"
            git push
          else
            echo "No hits to commit."
          fi
